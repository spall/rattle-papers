\section{Rattle}
\label{sec:rattle}

Our goal is to take a list of commands, and turn that into a build system. To do that, we assume the existence of a \emph{tracing oracle} which can, after a command completes, tell us which files that command read and which files it wrote -- we discuss the implementation and limitations of the tracing oracle in \S\ref{sec:tracing}. In this section we start by describing the most simple system possible, and progressively add features to obtain the benefits of a conventional build system.

\subsection{Executing commands}

In it's simplest variant, a build script can be a list of shell commands, like in \S\ref{sec:introduction}. To execute such commands correctly, it is sufficient to run each command sequentially in the order they were given. We consider this the reference semantics, and want any optimised/cached implementation to give the same results. For now, we assume commands are deterministic, but discuss how to weaken that assumption in \S\ref{sec:determinism}. We also assume that each command is atomic - it cannot be subdivided into smaller parts.

\subsection{Monadic builds}
\label{sec:monadic}

A build system which can only execute a static list of commands is restricted in its expressive power. Taking the build system from \S\ref{sec:introduction} - it would be better to compile and link \emph{all} \texttt{*.c} files -- not just those listed in the script. A more plausible script might be:

\begin{verbatim}
FILES=$(ls *.c)
for FILE in $FILES; do
    gcc -c $FILE
done
gcc -o main.exe $FILES{/.c/.o/}
\end{verbatim}

\begin{figure}
\begin{verbatim}
import Development.Rattle

main = runRattle $ do
    -- TODO: Convert
    FILES=$(ls *.c)
    for FILE in $FILES; do
        gcc -c $FILE
    done
    gcc -o main.exe $FILES{/.c/.o/}
\end{verbatim}
\caption{A Haskell/\Rattle version of the script from \S\ref{sec:monadic}}
\label{fig:monadic}
\end{figure}

Now we have a curious mixture of build system commands (e.g. \texttt{ls}, \texttt{gcc}), some control logic (e.g. \texttt{for}) and simple manipulation (e.g. changing file extension). The way we cope with this is to consider the build system as a series of commands, where the future commands may depend on the results of previous commands. These are glued together with ``cheap'' functions like the control logic and simple manipulations. We take the approach that the cheap commands are fixed overhead, run on every build, and not cached or parallelised in any way. If any of these cheap manipulations becomes expensive, they can be replaced by a command, which will then be cached and parallelised.

To implement \Rattle we treat the script as a sequence of commands with no observation on which commands are coming next. As a consequence, even if you are using a simple static script, and then manually edit it, the \Rattle build remains correct -- it has no knowledge that you manually edited the script, or if it was instead conditional on something it didn't observe.

\subsection{Skipping unnecessary commands}

When \Rattle runs a command, it records the files it reads and writes, and their hashes at that point. If you subsequently run the same command, and the inputs and outputs haven't changed (same hashes), it can be skipped. This statement is true for deterministic commands, but for nondeterministic ones you get old but consistent values. In practice this means that any command that isn't relied on to produce fresh entropy (the time, a GUID, a random number) is fine to skip subsequent times. This insight was pioneered by the \Fabricate\citep{fabricate} build system.

\subsection{Cloud builds}

If \Rattle matches on the files read, but not on the files written, but a previous run had the same files read/written and cached the written files, you can copy them over. If that store is in a cloud, you end up with cloud build functionality. Assuming commands that can be cached, whether a command uses a cloud oracle or runs locally is not observable to the rest of the system, so we don't need to consider it for the rest of the system. While this approach works beautifully in theory, in practice it leads to at least three separate problems:

\paragraph{Relative build directories} Often the current directory, or users profile directory, will be accessed by commands. These change either if the user has two working directories, or if they use different machines. We solve this by having a substitution table, replacing values such as the users home directory with \texttt{\$HOME}.

\paragraph{Compound commands} Sometimes a command will produce something that is user specific (not great for caching), but the next step will remove the user specificity (good for caching). To fix that we allow compound commands, by conjoining two commands with \texttt{\&\&}.

\paragraph{Non-deterministic builds} If a user builds something locally, they can leave the cache path. We solve that by allowing the user to specify different hashes to use, see \S\ref{sec:determinism}.

\subsection{Build consistency}

For a \Make build system to be stable, it must be the case that after a build, a rebuild will not execute any other commands. It's easy to construct examples of sequences of commands that violate this property, for example:

\begin{verbatim}
gcc -c foo.c
echo x >> foo.c
\end{verbatim}

This script compiles \texttt{foo.c}, then appends the character \texttt{x} to \texttt{foo.c}. Each time around, the input to \texttt{gcc} will have changed, necessitating a recompile. We define such a build as \emph{hazardous}, because it violates one of our consistency rules:

\begin{description}
\item[read-write hazard] When one command reads from a file, and a subsequent command writes to that file. On a future build, the first command will have to be rerun.
\item[write-write hazard] When two commands both write to the same file. On a future build, the first will be rerun (it's output has changed), which is likely to then cause the second to be rerun.
\end{description}

We assume that if a single command both reads and writes to the same file (as \texttt{echo x >> foo.c} does), then within that command they were correctly sequenced.

Using the tracing we spot hazards and raise errors if they occur. We prove that a build system with deterministic control logic with no hazards always results in no rebuilds in \S\ref{sec:proof:no_rebuild}. In a build system without hazards there is at most one write to any file, which occurs before any reads of that file. We can therefore prove there are no rebuilds by showing the first command can't rebuild, and proceeding by induction.

\subsection{Parallelism}

Given a script with no hazards when executed sequentially, we can show that any interleaving of those commands that also has no hazards will result in an equivalent output in \S\ref{sec:proof:reorder}. Moreover, any parallel execution without hazards will remain consistent, see \S\ref{sec:proof:parallel}. And even further, if we execute any additional commands that don't cause hazards, they can be shown to have no impact on the rest of the build, see \S\ref{sec:proof:additional}.

As a consequence of the above, it is important that we check for hazards, but are otherwise free to run things in parallel. There are two ways we allow parallelism.

\subsubsection{Explicit Parallelism}

In the Haskell API for \Rattle there is a parallel combinator \texttt{forP}. Replacing \texttt{forM} with \texttt{forP} in Figure \ref{fig:monadic} causes the commands to be given to \Rattle in parallel. As a consequence, they can be executed in parallel. The use of explicit parallelism is convenient when replacing a loop, but harder when expressing that three executables can be built in parallel, but that a single shared object common to all of them must complete before starting, so it is difficult to provide \emph{complete} parallelism annotations.

Interestingly, given complete \emph{dependency} information (e.g. as available to \Make) it is possible to infer complete \emph{parallelism} information. However, the difficult of writing such information is the attraction to a tracing approach in the first place.

\subsubsection{Implicit Parallelism}

The other source of parallelism is implicit, using speculation. If we can predict what commands are coming up next, and predict that their execution will not cause hazards, then we could speculatively execute them. Such predictions can be made by simply recording the last known execution and using that. If speculation suggests that running a command would be beneficial there are two possible approaches to take.

Firstly, we can execute the command remotely, or in a sandboxed manner - ensuring all writes do \emph{not} end up on the file system, but are recorded to the cloud cache. In such a mode we are filling up the coud cache speculative, with the hope that when the future command does arrive it can be satisfied from the cache. However, running remotely requires syncronising the files across, or using a sycronise on demand approach (CITE Microsoft Remote Execution talk). Running with a sandbox which intercepts writes is not an easily available cross-platform feature.

Alternatively, we can execute the command locally, which is what \Rattle does. However, if the execution leads to a hazard, it is possible that the hazard is entirely an artefact of speculation. One simplistic approach is to simply rerun without speculation if speculation leads to a hazard. A more refined approach is to determine whether classification may have impacted the hazard, which can be categorised as per \S\ref{sec:proof:classify_hazard}, and either raise the error immediately (if the speculation was not at fault) or selectively eliminte a subset of commands from speculation (if speculation was at fault).
